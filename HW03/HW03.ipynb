{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №3 \n",
    "## Обучение моделей глубокого обучения на PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача на этой неделе - повторить модель трёхслойного перцептрона из прошолго задания на PyTorch, разобрать лучшие практики обучения моделей глубокого обучения и провести серию экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Type, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы эксперимент можно было повторить, хорошей практикой будет зафиксировать генератор случайных чисел. Также, рекоммендуется зафиксировать RNG в numpy и, если в качестве бэкенда используется cudnn - включить детерминированный режим.\n",
    "\n",
    "Подробнее: https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "Основным способом организации кода на PyTorch является модуль. Простые модели могут быть реализованны из готовых модулей ( к примеру, torch.nn.Sequential, torch.nn.Linear и т.д. ), для более сложных архитектур часто приходтся реализовывать собственные блоки. Это достаточно легко сделать - достаточно написать класс, наследуемый от torch.nn.Module и реализующий метод .forward, который принимает и возвращает тензоры ( torch.Tensor )\n",
    "\n",
    "Пример реализации кастомного модуля из официальной документации: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "\n",
    "Повторите реализацию трёхслойного перцептрона из предыдущего задания на pytorch. Желательно также, чтобы реализация модели имела параметризуемую глубину ( количество слоёв ), количество параметров на каждом слое и функцию активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_resolution: Tuple[int, int] = (28, 28),\n",
    "                 input_channels: int = 1, \n",
    "                 hidden_layer_features: List[int] = [256, 256, 256],\n",
    "                 activation: Type[torch.nn.Module] = torch.nn.ReLU,\n",
    "                 num_classes: int = 10):\n",
    "\n",
    "        super().__init__()\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код позволяет посмотреть архитектуру получившейся модели и общее количество обучаемых параметров. Мы хотим, чтобы количество параметров в модели было порядка сотен тысяч - если у вас получается больше или меньше - попробуйте изменить архитектуру модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "print(model)\n",
    "print('Total number of trainable parameters', \n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучающая выборка\n",
    "\n",
    "На практике, наиболее важным для успеха обучения любой ML модели является этап подготовки данных. Модели глубокого обучения не являются исключением - большая, чистая и репрезантативная поставленной задаче обучающая выборка часто важнее, чем архитектура самой модели. В нашем случае мы используем качественный и проверенный временем MNIST, но в практических задачах часто будет получаться так, что лучшим способом добиться улучшения качества модели будет сбор дополнительных данных и очистка датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных\n",
    "\n",
    "Для улучшения сходимости алгоритма обучения и качества полученной модели данные должны быть предобработанны:\n",
    "\n",
    "1. Среднее каждой входной переменной близко к нулю\n",
    "2. Переменные отмасштабированы таким образом, что их дисперсии примерно одинаковы ( из соображений вычислительной устойчивости, мы хотим, чтобы все величины по порядку величины были близки к еденице )\n",
    "3. По возможности, входные переменные не должны быть скоррелированны. Важнось этого пункта в последние годы ставится под сомнение, но всё-же в некоторых случаях это может влиять на результат\n",
    "\n",
    "Подробнее можно почитать здесь: http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугументация обучающей выборки\n",
    "\n",
    "В зависимости от задачи, мы можем применять к обучающей выборке различные преобразования, которые позволят увеличить эффективный размер выборки без дополнительной разметки. К примеру, для задачи классификации кошек и собак мы можем зеркально отразить изображение вокруг вертикальной оси - при этом класс изображения не изменится, и само изображение останется по прежнему будет принадлежать исходному распределению. Есть много разных техник аугументации данных, и их применимость и эффективность сильно зависит от датасета и задачи. \n",
    "\n",
    "Подробнее можно почитать здесь: https://link.springer.com/content/pdf/10.1186/s40537-019-0197-0.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Обоснуйте, почему аугументация обучающей выборки позволяет добиться прироста качества модели, несмотря на то, что она не добавляет в неё дополнительную информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Какие осмысленные аугментации вы можете придумать для следующих наборов данных:\n",
    "\n",
    "1. Датасет изображений животных, размеченый на вид животного\n",
    "2. Датасет с аудиозаписями голоса, размечеными на язык говорящего\n",
    "3. Датасет cо значениями показаний датчиков температуры, влажности и давления с одной из метеостанций, размеченый на наличие или отсутствие осадков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "\n",
    "Напишите пайплайн для предобработки и аугументации данных. В torchvision.transforms есть готовые реализации большинства распространённых техник, если вы хотите добавить что-то своё, вы можете воспользоваться torchvision.transforms.Lambda. Также, для пайплайн для валидационной выборки не должен включать аугментации - мы хотим оценить качество модели на оригинальных данных.\n",
    "\n",
    "Одним из обязательных шагов в вашем пайплайне должен быть torchvision.transforms.ToTensor(), который сконвертирует PIL изображение в torch.Tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    # your core here\n",
    "])\n",
    "\n",
    "val_transforms = torchvision.transforms.Compose([\n",
    "    # your core here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./mnist', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=train_transforms)\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(root='./mnist', \n",
    "                                         train=False, \n",
    "                                         download=True, \n",
    "                                         transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как запускать обучение всегда стоит посмотреть на данные после предобработки, и удостовериться, что они соответствуют ожидаемым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0, len(train_dataset), size=256)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=16, ncols=16, figsize=(32, 32))\n",
    "for i, row in enumerate(axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        sample_index = indices[i*16+j]\n",
    "        sample, label = train_dataset[sample_index]\n",
    "        ax.imshow(sample.cpu().numpy().transpose(1, 2, 0))\n",
    "        ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "\n",
    "Теперь, когда мы реализовали модель и подготовили данные мы можем приступить к непосредственному обучению модели. Костяк функции обучения написан ниже, далее вы должны будете реализовать ключевые части этого алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module, \n",
    "                train_dataset: torch.utils.data.Dataset,\n",
    "                val_dataset: torch.utils.data.Dataset,\n",
    "                loss_function: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
    "                optimizer_class: Type[torch.optim.Optimizer] = torch.optim,\n",
    "                optimizer_params: Dict = {},\n",
    "                initial_lr = 0.01,\n",
    "                lr_scheduler_class: Any = torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                lr_scheduler_params: Dict = {},\n",
    "                batch_size = 64,\n",
    "                max_epochs = 1000,\n",
    "                early_stopping_patience = 20\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, **optimizer_params)\n",
    "    lr_scheduler = lr_scheduler_class(optimizer, **lr_scheduler_params)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_epoch = None\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        print(f'Epoch {epoch}')\n",
    "        train_single_epoch(model, optimizer, loss_function, train_loader)\n",
    "        val_metrics = validate_single_epoch(model, loss_function, val_loader)\n",
    "        print(f'Validation metrics: \\n{val_metrics}')\n",
    "\n",
    "        lr_scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        if best_val_loss is None or best_val_loss > val_metrics['loss']:\n",
    "            print(f'Best model yet, saving')\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_epoch = epoch\n",
    "            torch.save(model, './best_model.pth')\n",
    "            \n",
    "        if epoch - best_epoch > early_stopping_patience:\n",
    "            print('Early stopping triggered')\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "\n",
    "Реализуйте функцию производящую обучение сети на протяжении одной эпохи ( полного прохода по обучающей выборке ). На вход будет приходить модель, оптимизатор, функция потерь и DataLoader. При итерировании по data_loader вы будете получать пары вида ( данные, класс )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model: torch.nn.Module,\n",
    "                       optimizer: torch.optim.Optimizer, \n",
    "                       loss_function: torch.nn.Module, \n",
    "                       data_loader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6\n",
    "\n",
    "Реализуйте функцию производящую рассчёт функции потерь на валидационной выборке.  На вход будет приходить модель, функция потерь и DataLoader. На выходе ожидается словарь с вида:\n",
    "```\n",
    "{\n",
    "    'loss': <среднее значение функции потерь>,\n",
    "    'accuracy': <среднее значение точности модели>\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_epoch(model: torch.nn.Module,\n",
    "                          loss_function: torch.nn.Module, \n",
    "                          data_loader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы корректно реализовали все предыдущие шаги и ваша модель имеет достаточное количество обучаемых параметров, то в следующей ячейке должен пойти процесс обучения, и мы должны достичь итоговой accuracy выше 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, \n",
    "            train_dataset=train_dataset, \n",
    "            val_dataset=val_dataset, \n",
    "            loss_function=torch.nn.CrossEntropyLoss(), \n",
    "            initial_lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7\n",
    "\n",
    "Попытайтесь модифицировать процесс обучения таким образом, чтобы достигнуть наилучшего качества на валидационной выборке. Модель должна оставаться N-слойным перцептроном с количеством обучаемых параметров <= 500000. Для обучения разрешается использовать только датасет MNIST. Процесс обучения вы можете изменять по собственному усмотрению. К примеру, вы можете менять:\n",
    "\n",
    "* Архитектуру модели, в рамках наложенных ограничений\n",
    "* Функции активации в модели\n",
    "* Используемый оптимизатор\n",
    "* Расписание LR\n",
    "* Сэмплинг данных при обучении ( e.g. hard negative mining)\n",
    "\n",
    "В результате мы ожидаем увидеть код экспериментов и любые инсайты, которые вы сможете получить в процессе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
