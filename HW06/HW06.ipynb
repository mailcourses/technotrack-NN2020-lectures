{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №6 - Анализ настроя отзыва с использованием LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой работе предлагается реализовать рекуррентную нейронную сеть, которая выполняет задачу определения настроя отзыва на фильм.\n",
    "\n",
    "Здесь использвается набор данных отзывов на фильмы, каждый из которых помечен как положительный или отрицательный.\n",
    "\n",
    "<img src=\"assets/reviews_ex.png\" width=40%>\n",
    "\n",
    "### Архитектура нейросети\n",
    "\n",
    "Ниже показана архитектура нейросети, которую предлагается реализовать.\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=40%>\n",
    "\n",
    "**Первым делом каждое слово отправляется на вход слоя извлечения скрытого представления (эмбеддингов)** Эмбеддинги нужны для того, чтобы десятки тысяч слов, которые потенциально могут быть использованы в отзывах, представить в более эффективном виде, нежели one-hot представление. Вы знакомы с понятием эмбеддингов, например, из лекции про Word2Vec. Эмбеддинги можно тренировать предварительно в подходе Word2Vec или использовать любую другую модель извлечения эмбеддингов. Однако в этой задаче достаточно вставить слой преобразования слов в эмбеддинги, чтобы на основании предоставленных на обучение данных он сам выучил наиболее эффективное скрытое представлени слов. Здесь слой извлечение эмбеддингов - скорее для снижения размерности, нежели для извлечения значимого семантического представления.\n",
    "\n",
    "**После того, как отдельные слова переданы в слой извлечения эмбеддингов, эти скрытые представления слов подаются на ячейку LSTM.** Ячейки LSTM - элементы рекуррентной нейронной сети, которые дают сети возможность усваивать информацию о последовательности слов в отзыве.\n",
    "\n",
    "**Выходным слоем после LSTM будет сигмоидальная активация.** Поскольку в случае двух взаимоисключающих исходов для каждого объекта (позитивный и негативный настрой отзыва) - можно решать задачу бинарной классификации. Для такой задачи финальная активация - сигмоид.\n",
    "\n",
    "Результат выдается ячейкой LSTM на каждом слове, однако нас интересует только финальный ответ - то есть, активация на последнем слове. Все предыдущие можно игнорировать. Соответственно, функция потерь (бинарная перекрестная энтропия) вычисляется для всего высказывания (отзыва) в целом, на значении активации на последнем слове."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Загрузим и отобразим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных\n",
    "\n",
    "Первым делом в решении подобных задач следует преобразовать данные к виду, который можно подавать на вход нейросети. Мы решили использовать обучаемый слой извлечения эмбеддингов, поэтому каждое слово следует закодировать. В этой работе предлагается закодировать слово просто номером в словаре. Для составления словаря, данные следует почистить:\n",
    "\n",
    "* Удалить знаки препинания\n",
    "* Привести все слова в нижний регистр\n",
    "* Отзывы разделены знаком переноса строки `\\n`. Следует разбить весь текст на отдельные отзывы по этому символу (отдельные отзывы понадобятся позже)\n",
    "* Объединить все отзывы без лишних символо в один длинный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "# Удалить знаки препинания\n",
    "reviews = reviews.lower() # перевести в нижний регистр\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделить на отдельные отзывы\n",
    "reviews_split = all_text.split('\\n')\n",
    "\n",
    "# объединить в один большой текст\n",
    "all_text = ' '.join(reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать список всех слов\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование слов\n",
    "\n",
    "Таблица соответствий (lookup-table) должна давать возможность идентифицировать каждое слово слову целым числом. Самый простой способ это сделать - создать словарь, ключами которого будут слова, значениями - целые числа. С использованием такой таблицы соответствий можно будет преобразовать все объекты выборки (отзывы на фильмы) в последовательность целых чисел.\n",
    "\n",
    "> **ЗАДАНИЕ 1:** Здесь следует закодировать слова целыми числами. Для упрощения извлечения уникальных слов и частоты их повторяемости можно использовать класс `Counter` модуля `collections` и его возможности сортировки по частоте элементов. Обратите внимание, что в дальнейшем потребуется использовать специальный символ отступа в последовательности, который будет кодироваться нулем `0`, поэтому при составлении таблицы соответствий **следует начинать нумерацию с `1`, а не с `0`**.\n",
    "> Далее преобразуйте все обзоры, записанные в переменной reviews, согласно сформированным целочисленным кодам. Запишите сконвертированные обзоры в новый список `reviews_ints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "vocab_to_int = None\n",
    "\n",
    "reviews_ints = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Протестируйте свой код**\n",
    "\n",
    "Проверьте количество уникальных слов в полученном словаре. Должно получиться более 74'000\n",
    "\n",
    "Также можно посмотреть, как выглядит произвольный из токенизированных обзоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование целевой переменной\n",
    "\n",
    "Метки обзоров имеют два возможных значения: \"positive\" или \"negative\". Чтобы использовать эти метки, их следует преобразовать к виду бинарной переменной.\n",
    "\n",
    "> **ЗАДАНИЕ 2:** Преобразуйте данные целевой переменной из слов `positive` и `negative` в бинарное значени (`1` и `0` соответственно). Запишите данные в массив целевой переменной  `encoded_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Преобразование меток отзывов: 1=positive, 0=negative\n",
    "encoded_labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация выбросов\n",
    "\n",
    "Длина обзоров, подаваемых на вход нейросети, должна будет быть одинаковая. Недостаток слов легко заполняется токеном-пропуском. Однако если отзыв необычно длинный, имеет смысл исключить его из рассмотрения. Это можно сделать следующим образом:\n",
    "\n",
    "1. Исключить экстремально длинные и экстремально короткие отзывы\n",
    "2. Заполнение недостающих (в смысле длины отзыва) слов специальным токеном-пропуском, чтобы все отзывы были одинаковые по длине.\n",
    "3. Уменьшение длины отзывов, превышающих установленный (вами) размер.\n",
    "\n",
    "<img src=\"assets/outliers_padding_ex.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом следует исключить из выборки экстремально длинные и экстремально короткие отзывы. Как минимум следует исключить отзывы с нулевой длиной.\n",
    "\n",
    ">**ЗАДАНИЕ 3:** Следует отфильтровать отзывы по длине. Например, можно удалить из `reviews_ints` отзывы длиной короче перцентиля уровня 1% и длинее перцентиля уровня 99%. Запишите отфильтрованные таким образом отзывы снова в `reviews_ints`, а соответствующие им метки - в список `encoded_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outlier review stats\n",
    "review_lengths = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "\n",
    "reviews_ints = # YOUR CODE HERE\n",
    "encoded_labels = # YOUR CODE HERE\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Заполнение недостающих токенов (padding)\n",
    "\n",
    "Для того, чтобы обрабатывать отзывы, превышающие задаваемую (вами) длину или не заполняющую ее целиком, следует применить заполнение недостающих токеном (т.н. padding) и отбросить лишние токены отзыва. Таким образом, все отзывы будут одной длины. Для отзывов, которые короче, чем некоторый `seq_length`, слева следует добавить ровно столько токенов пустого слова (то есть, `0`), чтобы их длина стала равной `seq_length` (\"padding\"). Для слишком длинный отзывов можно отбросить последние токены таким образом, чтобы их длина также сравнялась с `seq_length` (\"truncate\"). Предлагаем использовать `seq_length`, равный 200. Однако этот параметр можно попробовать варьировать.\n",
    "\n",
    "> **ЗАДАНИЕ 4:** Реализовать преобразование (padding + truncate) в виде фунации, которая бы возвращала массив целочисленных признаков слов отзывов (названы `features` в заготовке кода ниже). При этом все отзывы по результату выполнения функции должны становиться одинаковой длины `seq_length`.\n",
    "\n",
    "* Данные отзывов берутся из набора закодированных отзывов `review_ints`;\n",
    "* Каждый массив закодированого отзыва должен получиться длины `seq_length`;\n",
    "* Для отзывов, которые короче `seq_length`, применяется левое дополнение (\"left pad\") пустым токеном `0`. То есть, закодированный отзыв длиной 3 вида `[117, 18, 128]` должен быть преобразован к виду `[0, 0, 0, ..., 0, 117, 18, 128]`;\n",
    "* Для отзывов, которые длинее `seq_length`, отбрасываются правые (последние) токены таким образом, чтобы результирующая длина составляла `seq_length`.\n",
    "\n",
    "**ПРИМЕР**\n",
    "\n",
    "Пусть `seq_length=10`. Тогда для отзыва, закодированного в виде: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "Результат должен выглядеть следующим образом: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "В конечном итоге, массив признакового описания отзывов `features` должен быть 2D массивом. Количество строк должно совпадать с количеством отзывов в `review_ints`. Количество колонок должно быть `seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    features=None\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Протестируйте ваш код\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## здесь тестируется результат - НЕ МЕНЯЙТЕ ЭТОТ КОД - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# первые 10 значений первых 30 мини-батчей\n",
    "print(features[:30,:10])\n",
    "## здесь тестируется результат - НЕ МЕНЯЙТЕ ЭТОТ КОД - ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на выборки: тренировочную, валидационную и тестовую.\n",
    "\n",
    "Теперь пора приступить к обычной процедуре разбиения доступной выборки данных на подвыборки для тренировки, валидации и тестирования.\n",
    "\n",
    "> **ЗАДАНИЕ 5:** разбить данные на три выборки: тренировочная, валидационная и тестовая. \n",
    "* Следует создать тренировочную подвыборку объемом `split_frac` от всей доступной.\n",
    "* оставшиеся данные разбейте пополам на валидационную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## Разбейте данные на выборки: тренировочную, валидационную и тестовую\n",
    "\n",
    "\n",
    "## Покажите размеры полученных выборок\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Загрузчик данных (DataLoader) и разбиение на мини-батчи\n",
    "\n",
    "Далее следует создать загрузчик данных для каждой из подвыборок. Для этого можно воспользоваться [TensorDataset](https://pytorch.org/docs/stable/data.html#) из массивов `numpy`, которые были созданы. Далее можно воспользоваться классом DataLoader, чтобы разбить полученные наборы данных на мини-батчи и организовать порождение мини-батчей.\n",
    "\n",
    "\n",
    "Например, это может выглядеть так:\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# не забудьте перемешать данные!\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Отобразите один элемент из потока порождаемых мини-батчей\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Искусственная нейронная сеть, определяющая настрой отзыва\n",
    "\n",
    "Теперь настало время реализовать нейросеть, которая будет собственно выполнять задачу.\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=40%>\n",
    "\n",
    "Предлагается реализовать нейросеть в следующем составе слоев:\n",
    "1. Слой извлечения эмбеддингов ([embedding layer](https://pytorch.org/docs/stable/nn.html#embedding)). Этот слой преобразует токены слов (целые числа) в эмбединги задаваемого размера;\n",
    "2. [Слой LSTM](https://pytorch.org/docs/stable/nn.html#lstm). Для этого слоя следует задать размер скрытого состояния `hidden_state` и количество слоев;\n",
    "3. Полносвязный слой на выходе LSTM, который преобразует выход LSTM в переменную задаваемого размера `output_size` (напомним, в этой задаче целевая переменная - только одно значение).\n",
    "4. Сигмоидальная активация. Напомним, что нейросеть должна возвращать вывод **только последнего шага рекуррентной ячейки**.\n",
    "\n",
    "### Слой извлечения эмбеддингов\n",
    "\n",
    "В словаре, сформированном в этой задаче, - более 74000 слов. Для сокращения размерности и повышения эффективности вычислений следует применять слой извлечения эмбеддингов слов, [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding). Можно было бы тренировать этот слой методом, аналогичным Word2Vec. Однако конкретно в этой задаче можно тренировать этот слой одновременно со всей нейросетью, в подходе end-to-end.\n",
    "\n",
    "\n",
    "### Слои LSTM\n",
    "\n",
    "Чтобы нейросеть была собственно рекуррентной, можно, например, использовать слои [LSTM](https://pytorch.org/docs/stable/nn.html#lstm). Изучите по документации назначение параметров этой ячейки! Можно экспериментировать с количеством слоев ячейки. Скорее всего, при повышении количества слоев `n_layers` ячейка будет способна усваивать и описывать более сложные закономерности в данных.\n",
    "\n",
    "> **ЗАДАНИЕ 6:** Реализуйте методы `__init__`, `forward`, и `init_hidden` в классе модели SentimentRNN.\n",
    "\n",
    "Заметим, что в методе `init_hidden` все скрытые состояния и состояния ячеек LSTM следует обнулить. Если есть возможность, эти состояния следует отправить на GPU в этом же методе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проверка, доступен ли GPU\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс модели рекуррентной сети, предназначенной для выполнения бинарной классификации текстов\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Конфигурация модели: создание объектов слоев сети\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define all layers\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Вычисление сети на некоторых объектах выборки и скрытых состоянии\n",
    "        \"\"\"\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Инициализация скрытых состояний '''\n",
    "        # Создать два новых тензора размеров [n_layers, batch_size, hidden_dim]\n",
    "        # для скрытых состояний и состояний ячейки. LSTM\n",
    "        # Обнулить созданные тензоры и (при необходимости) отрпавить их на GPU\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание экземпляра класса нейросети\n",
    "\n",
    "> **ЗАДАНИЕ 7:** Следует создать экземпляр класса описанной выше нейросети с заданием следующих гиперпараметров:\n",
    "\n",
    "* `vocab_size`: Размер словаря (для слоя эмбеддингов)\n",
    "* `output_size`: Размер вектора результата на нейросети (1 в случае бинарной классификации)\n",
    "* `embedding_dim`: Количество признаков векторов эмбеддингов\n",
    "* `hidden_dim`: Количество \"нейронов\" скрытого слоя ячейки LSTM. Обычно чем больше, - тем лучше. Значения, которые можно рассматривать как первое приближение: 128, 256, 512, *etc.*\n",
    "* `n_layers`: Количество слоев LSTM. Неплохим первым приближением может быть количество 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = \n",
    "output_size = \n",
    "embedding_dim = \n",
    "hidden_dim = \n",
    "n_layers = \n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Обучение нейросети\n",
    "\n",
    "Здесь код не будет отличаться от того, к чему вы уже привыкли. Здесь следует реализовать стандартный цикл обучения.\n",
    "\n",
    "> **ЗАДАНИЕ 8:** реализовать цикл обучения нейросети с периодической проверкой качества на валидационной выбрке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция потерь и оптимизатор\n",
    "criterion = None\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        output, h = net(inputs, h)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Batch: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Тестирование нейросети\n",
    "\n",
    "Тестировать обученную нейросеть можно несколькими способами.\n",
    "\n",
    "* **В мере качества, оцениваемой на тестовой выборке**\n",
    "\n",
    "* **Оценка правдоподобности результата на отзывах, написанных исследователем.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ЗАДАНИЕ 9:** оцените качество модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = None\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # output -> label\n",
    "    pred = None\n",
    "    \n",
    "    # Вычисление Accuracy\n",
    "    \n",
    "\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение модели на тестовом отзыве\n",
    "\n",
    "> **ЗАДАНИЕ 10:** реализовать фунацию, `predict`, которая принимает на вход обученную сеть, текст отзыва, длину последовательности `seq_length`, кодирует текст в последовательность целочисленных кодов, применяет  padding и truncate и выводит результат, является ли настрой отзыва положительным или отрицательным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отрицательный отзыв\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Положительный отзыв\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Примените фунацию. Какой ответ выдает нейросеть?\n",
    "# Попробуйте оба отзыва. Попробуйте отызвы, которые сможете найти в Интернете.\n",
    "seq_length=200\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
