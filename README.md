# Нейронные сети: новый уровень
Преподаватели:

[Святослав Елизаров](https://github.com/DukeGonzo) ([github](https://github.com/DukeGonzo))

[Михаил Криницкий](https://sail.ocean.ru/viewuser.php?user=krinitsky) ([RG link](https://www.researchgate.net/profile/Mikhail_Krinitskiy), [github](https://github.com/MKrinitskiy))



Обратите внимание на [правила распространения источников и литературы](https://github.com/MKrinitskiy/ML4ES_2019-2020/blob/master/resources_policy.md)<br />

[Здесь](https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189) доступна подборка книг издательства Springer, доступных для свободной загрузки, по компьютерным наукам, компьютерному зрению, машинному обучению и науке о данных. [Здесь](https://link.springer.com/search/page/3?facet-content-type="Book"&package=openaccess) - полный список книг издательства Springer, выложенных в открытый доступ для свободной загрузки.


## Материалы занятий

| Title | Date | Topic | Content |
| ----- | ---- | ----- | ------- |
| Лекция 1 | 17.09.2020 | Вводная лекция.<br />От линейных моделей к нейронным сетям. Часть 1 | [материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect01);<br />[видеозапись](https://www.dropbox.com/s/aucsed48shivyex/Lect01-record.mp4?dl=0) |
| Лекция 2 | 24.09.2020 | Вероятностные основы линейных моделей. <br />Обобщенные линейные модели. Перцептрон. | [материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect02);<br />[видеозапись](https://www.dropbox.com/s/n7dm1uwjnot4rz5/Lect02-record.mp4?dl=0) |
| Лекция 3 | 01.10.2020 | Технические аспекты вычисления градиента функции потерь глубоких нейронных сетей.<br />Особенности оптимизации функции потерь. | [видеозапись](https://www.dropbox.com/s/go3bqm9az8zwmuz/Lect03-record.mp4?dl=0)<br />[материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/HW03)|
| Лекция 4 | 08.10.2020 | Особенности оптимизации функции потерь (продолжение)<br />Вопросы инициализации параметров. | [видеозапись](https://www.dropbox.com/s/exlrippv0qum5au/Lect04.mp4?dl=0)<br />[материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect04) |
| Лекция 5 | 15.10.2020 | Особенности оптимизации функции потерь (продолжение)<br />Пакетная нормализация и регуляризации. | [видеозапись](https://www.dropbox.com/s/sojxmymazp66eks/Lect05.mp4?dl=0)<br />[материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect05) |
| Лекция 6 | 22.10.2020 | Циркулянт и операция дискретной циклической свертки<br />Сверточные нейронные сети. | [видеозапись](https://www.dropbox.com/s/7qvoh53y2g5udnp/Lect06.mp4?dl=0)<br />[материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect06) |
| Лекция 7 | 29.10.2020 | Практика<br />Современные архитектуры сверточных нейронных сетей | [видеозапись](https://www.dropbox.com/s/m727gp80hut6kel/Lect07.mp4?dl=0) |
| Лекция 8 | 05.11.2020 | Визуализация и интерпретация сверточных нейронных сетей<br />Снижение размерности и автокодировщики | [видеозапись](https://www.dropbox.com/s/hhzph686us2614z/Lect08.mp4?dl=0) |
| Лекция 9 | 05.11.2020 | Визуализация и интерпретация сверточных нейронных сетей<br />Снижение размерности и автокодировщики | [видеозапись (TBA)]()<br />[материалы](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/Lect09) |


## Домашние задания
| Title | Date issued |  Deadline  | Topic | Content |
| ----- | ----------- | ---------- | ----- | ------- |
| ДЗ №1 | 18.09.2020  | 24.09.2020 | Градиентный спуск своими руками | [описание и данные](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/HW01) |
| ДЗ №2 | 27.09.2020  | 14.10.2020 | Обучение трехслойного перцептрона | [описание](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/HW02) |
| ДЗ №3 | 10.10.2020  | 17.10.2020 | Оптимизация многослойного перцептрона на PyTorch | [описание](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/HW03) |
| ДЗ №4 | 23.10.2020  | 30.10.2020 | Обзор последних исследовательских статей по искусственным нейронным сетям | [Описание и список статей](https://github.com/mailcourses/technotrack-NN2020-lectures/tree/master/HW04) |


### Рекомендуемая литература

(следите за обновлениями!)

- *Николенко С. И., Кадурин А. А., Архангельская Е. О.* "Глубокое обучение." / СПб.: Питер. 2019. 480 с. [publisher link](https://www.piter.com/product/glubokoe-obuchenie)
- *Гудфеллоу Я., Бенджио И., Курвилль А.* "Глубокое обучение." / М.: ДМК Пресс, 2017. 652 c. [publisher link](https://dmkpress.com/catalog/computer/data/978-5-97060-554-7/)
- Интерактивная онлайн-книга *Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola* ["Dive into Deep Learning"](http://d2l.ai/) 

Дополнительные источники

- *Флах П.* "Машинное обучение. Наука и искусство построения алгоритмов, которые извлекают знания из данных." / Флах П. М.: ДМК Пресс, 2015. 400 c. [publisher link](https://dmkpress.com/catalog/computer/data/978-5-97060-273-7/)

- *Шай Шалев-Шварц, Шай Бен-Давид* "Идеи машинного обучения." / - М.: ДМК Пресс, 2018. 432 c.

- [*Hastie T., Tibshirani R., Friedman J.* "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition"](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) / T. Hastie, R. Tibshirani, J. Friedman, 2-е изд., New York: Springer-Verlag, 2009.

  ​	Первод на русский язык: ([Фридман Дж., Хасти Т., Тибширани Р. "Основы статистического обучения"](http://www.combook.ru/product/11965387/))

- *Bishop C.* "Pattern Recognition and Machine Learning" / C. Bishop, New York: Springer-Verlag, 2006. [Доступна для легального скачивания.](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

  ​	Перевод на русский язык: [Бишоп К.М. "Распознавание образов и машинное обучение"](http://www.combook.ru/product/11965388/)

- [Matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) - справочник по соотношениям в матричной форме

- Курс лекций [К.В. Воронцова](http://www.machinelearning.ru/wiki/index.php?title=%D0%A3%D1%87%D0%B0%D1%81%D1%82%D0%BD%D0%B8%D0%BA:%D0%9A%D0%BE%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%82%D0%B8%D0%BD_%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2):
  - [Математические методы обучения по прецедентам](http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf)
  - [Оценивание и выбор моделей](http://www.machinelearning.ru/wiki/images/2/2d/Voron-ML-Modeling.pdf)
  - [Логические алгоритмы классификации](http://www.machinelearning.ru/wiki/images/3/3e/Voron-ML-Logic.pdf)
  - [Алгоритмические композиции](http://www.machinelearning.ru/wiki/images/0/0d/Voron-ML-Compositions.pdf)
  
- Курс лекций [Л.М. Местецкого](http://www.machinelearning.ru/wiki/index.php?title=%D0%A3%D1%87%D0%B0%D1%81%D1%82%D0%BD%D0%B8%D0%BA:Mest) ["Математические методы распознавания образов"](http://www.ccas.ru/frc/papers/mestetskii04course.pdf)

- Препринт книги Cosma Rohilla Shalizi "Advanced Data Analysis from an Elementary Point of View". [Доступен онлайн](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/).

- *James G., Witten D., Hastie T., Tibshirani R.,* 2013. "An Introduction to Statistical Learning: with Applications in R", Springer Texts in Statistics. Springer-Verlag, New York. Книга [доступна для скачивания](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf).